# -*- coding: utf-8 -*-
"""1-ref_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nyT4oQ-GlOtUhPPX6UFZ-u0WaVBGIpWD
"""

!sudo apt-get install python3.9

!pip install numpy==1.23.5

import pandas as pd
df = pd.read_csv('/content/dataset_srip_co.csv')
df.head()
df= df.drop(columns=['Unnamed: 0'])

!pip install datasets --upgrade

from datasets import load_dataset
from datasets import Dataset
#dataset = load_dataset("csv",data_files="/content/dataset_srip_co.csv", split='train',delimiter =";")
#dataset= load_dataset("json",data_files="/content/dataset_srip.json", split='train')
# Explore the data
#df_train = data_train.to_pandas()
dataset = Dataset.from_pandas(df)
data_train_ = dataset.train_test_split(test_size=0.2)
data_train=data_train_['train']
data_test=data_train_['test']
df_test= data_test.to_pandas()
df_train = data_train.to_pandas()
df_train.head(10)

!pip install -q -U bitsandbytes
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -I git+https://github.com/huggingface/peft.git
!pip install -q -U accelerate --upgrade
!pip install -q trl

from huggingface_hub import notebook_login
notebook_login()

!pip install -U huggingface_hub

!pip install bitsandbytes --upgrade

import torch
from transformers import AutoTokenizer
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_id="mistralai/Mistral-7B-v0.1"
from datasets import load_dataset
from peft import prepare_model_for_kbit_training
from peft import LoraConfig, get_peft_model
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto")
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})
tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)

data_train = data_train.shuffle(seed=1234)  # Shuffle dataset here
data_train = data_train.map(lambda samples: tokenizer(samples["prompt"]), batched=True)

data_test=data_test.shuffle(seed=1234)
data_test = data_test.map(lambda samples: tokenizer(samples["prompt"]), batched=True)
#data = data.train_test_split(test_size=0.1)
train_data = data_train
test_data = data_test

prompt_template = """
  Below is an instruction that describes a task. Write a response that appropriately completes the request.
  ### Question:
  {query}

  ### Answer:
  """

def generate_prompt_2(data_point, return_out=True):
    """Gen. input text based on a prompt, task instruction, (context info.), and answer

    :param data_point: dict: Data point
    :return: dict: tokenzed prompt
    """

    text = 'Given Input in the form of questions, Generate a Paraphrase such that it conveys the same meaning as Input.\n\n'
    text += f'### Input:\n{data_point}\n\n'
    if(return_out):
      text += f'### Paraphrase:\n{data_point["variation_1"] if return_out else ""}'


    return text



def generate_prompt(data_point, return_out=True):
    """Gen. input text based on a prompt, task instruction, (context info.), and answer

    :param data_point: dict: Data point
    :return: dict: tokenzed prompt
    """

    text = 'Given Input in the form of text which poses a question, Generate Paraphrases such that it conveys the same meaning as the Input.\n\n'
    text += f'### Input:\n{data_point["question"]})\n\n'
    text += f'### Paraphrase:\n{data_point["variation_1"] if return_out else ""}'



    return text

# add the "prompt" column in the dataset
text_column = [generate_prompt(data_point) for data_point in data_train]
data_train = data_train.add_column("prompt", text_column)


text_column = [generate_prompt(data_point) for data_point in data_test]
data_test = data_test.add_column("prompt", text_column)

data_train[0]["prompt"]

from peft import prepare_model_for_kbit_training

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj","o_proj","gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

peft_model = get_peft_model(model, lora_config)
print_trainable_parameters(peft_model)

model.add_adapter(lora_config, adapter_name="adapter")

#Here I reload the model and specify it should be loaded in a single GPU to avoid errors" Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! when resuming training"
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})

#new code using SFTTrainer
import transformers

from trl import SFTTrainer

tokenizer.pad_token = tokenizer.eos_token
torch.cuda.empty_cache()

trainer = SFTTrainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=test_data,
    dataset_text_field="prompt",
    peft_config=lora_config,
    args=transformers.TrainingArguments(
        warmup_steps=5,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        max_steps=1000,
        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate
        logging_steps=50,
        #bf16=True,
        #fp16=True,
        optim="paged_adamw_8bit",
        logging_dir="./logs",        # Directory for storing logs
        save_strategy="steps",       # Save the model checkpoint every logging step
        save_steps=50,                # Save checkpoints every 50 steps
        evaluation_strategy="steps", # Evaluate the model every logging step
        eval_steps=50,               # Evaluate and save checkpoints every 50 steps
        do_eval=True,
        output_dir="outputs_mistral",
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
trainer.train()

torch.save(model.state_dict(),"/content/paraphrasing_10ref.pt")

from google.colab import drive

# Mount your Google Drive to the Colab VM.
drive.mount('/content/drive')

# Copy the model file to your Google Drive.
!cp /content/paraphrasing_10ref.pt /content/drive/MyDrive

# Print the contents of your Google Drive to verify the upload.
!ls /content/drive/MyDrive

def get_completion_2(query: str, model, tokenizer) -> str:
  device = "cuda:0"

  prompt = generate_prompt_2(query, return_out=False)
  print(prompt)

  encodeds = tokenizer(prompt, return_tensors="pt", )

  model_inputs = encodeds.to(device)

  generated_ids = model.generate(**model_inputs, max_new_tokens=230, do_sample=True)
  decoded = tokenizer.batch_decode(generated_ids)
  print(decoded)
  #return (decoded[0][327+len(query["input"])])
  return (decoded)

data_test

el="What is a SVG element in HTML?"
result = get_completion_2(query=el, model=model, tokenizer=tokenizer)
print(result)

from datasets import load_metric
with open('/content/results_mistral_10ref.txt','w') as f:
  for el in data_test:
    result = get_completion_2(query=el['question'], model=model, tokenizer=tokenizer)
  #print(result,result[327+len(data_test[0]["input"])],"actual answer being",d["output"])
    print(result)
    f.write(result[0])
    f.write('---------------------------------')
    '''
    bleu = load_metric("bleu")
    rouge = load_metric("rouge")
    meteor = load_metric("meteor")
    #result = get_completion_2(el, model, tokenizer)


    bleu_score = bleu.compute(predictions=[result[0]], references=el)['bleu']
    rouge_score = rouge.compute(predictions=[result[0]], references=el)['rougeLsum']
    meteor_score = meteor.compute(predictions=[result[0]], references=el)['meteor']

    f.write(f"BLEU score: {bleu_score}\n")
    f.write(f"ROUGE score: {rouge_score}\n")
    f.write(f"METEOR score: {meteor_score}\n")
    f.write('\n')
    print(f"BLEU score: {bleu_score}")
    print(f"ROUGE score: {rouge_score}")
    print(f"METEOR score: {meteor_score}")'''

with open('/content/test_SRIP.txt','r') as f:
  data_test = f.readlines()

for el in data_test:
  print(el)

with open('/content/results_mistral_10ref_test.txt','w') as f:

  for el in data_test:
    result = get_completion_2(query=el, model=model, tokenizer=tokenizer)
  #print(result,result[327+len(data_test[0]["input"])],"actual answer being",d["output"])
    print(result)
    f.write(result[0])
    f.write('---------------------------------')

